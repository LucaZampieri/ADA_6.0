{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data from the Web\n",
    "\n",
    "## Deadline\n",
    "Wednesday October 25, 2017 at 11:59PM\n",
    "\n",
    "## Important Notes\n",
    "* Make sure you push on GitHub your Notebook with all the cells already evaluated (i.e., you don't want your colleagues to generate unnecessary Web traffic during the peer review)\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and the solution you plan to implement!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "## Background\n",
    "In this homework we will extract interesting information from www.topuniversities.com and www.timeshighereducation.com, two platforms that maintain a global ranking of worldwide universities. This ranking is not offered as a downloadable dataset, so you will have to find a way to scrape the information we need!\n",
    "You are not allowed to download manually the entire ranking -- rather you have to understand how the server loads it in your browser. For this task, Postman with the Interceptor extension can help you greatly. We recommend that you watch this [brief tutorial](https://www.youtube.com/watch?v=jBjXVrS8nXs&list=PLM-7VG-sgbtD8qBnGeQM5nvlpqB_ktaLZ&autoplay=1) to understand quickly how to use it.\n",
    "\n",
    "## Assignment\n",
    "1. Obtain the 200 top-ranking universities in www.topuniversities.com ([ranking 2018](https://www.topuniversities.com/university-rankings/world-university-rankings/2018)). In particular, extract the following fields for each university: name, rank, country and region, number of faculty members (international and total) and number of students (international and total). Some information is not available in the main list and you have to find them in the [details page](https://www.topuniversities.com/universities/ecole-polytechnique-fédérale-de-lausanne-epfl).\n",
    "Store the resulting dataset in a pandas DataFrame and answer the following questions:\n",
    "- Which are the best universities in term of: (a) ratio between faculty members and students, (b) ratio of international students?\n",
    "- Answer the previous question aggregating the data by (c) country and (d) region.\n",
    "\n",
    "Plot your data using bar charts and describe briefly what you observed.\n",
    "\n",
    "2. Obtain the 200 top-ranking universities in www.timeshighereducation.com ([ranking 2018](http://timeshighereducation.com/world-university-rankings/2018/world-ranking)). Repeat the analysis of the previous point and discuss briefly what you observed.\n",
    "\n",
    "3. Merge the two DataFrames created in questions 1 and 2 using university names. Match universities' names as well as you can, and explain your strategy. Keep track of the original position in both rankings.\n",
    "\n",
    "4. Find useful insights in the data by performing an exploratory analysis. Can you find a strong correlation between any pair of variables in the dataset you just created? Example: when a university is strong in its international dimension, can you observe a consistency both for students and faculty members?\n",
    "\n",
    "5. Can you find the best university taking in consideration both rankings? Explain your approach.\n",
    "\n",
    "Hints:\n",
    "- Keep your Notebook clean and don't print the verbose output of the requests if this does not add useful information for the reader.\n",
    "- In case of tie, use the order defined in the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import collections\n",
    "import string\n",
    "import unidecode # to remove accents\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a request\n",
    "url_main = 'https://www.topuniversities.com'  # Found with postman\n",
    "r = requests.get(url_main + '/sites/default/files/qs-rankings-data/357051.txt')\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "page_body = r.text\n",
    "\n",
    "# Serialize the json data with json library\n",
    "rank_json = json.loads(page_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dataframe\n",
    "rank_df = pd.DataFrame()\n",
    "rank_df = rank_df.from_dict(rank_json['data']).head(200)\n",
    "rank_df.stars\n",
    "rank_df.drop(['logo', 'stars', 'nid','cc', 'score'], axis=1, inplace=True)\n",
    "rank_df.set_index('core_id', inplace=True)\n",
    "rank_df = rank_df[['title', 'rank_display', 'country', 'region', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_find(html_attributes, new_df_column_name, rank_df):\n",
    "    _tag = html_attributes['tag']\n",
    "    _class = html_attributes['class']\n",
    "    # _list is a temporary list that will store the values found and then be converted in the df to be returned.\n",
    "    _list = []\n",
    "    for url in rank_df.url:\n",
    "        # for every url contained in rank_df['url'], perform the corresponding request:\n",
    "        uni_url = requests.get(url_main + url)\n",
    "        uni_body = uni_url.text\n",
    "        soup = BeautifulSoup(uni_body, 'html.parser')\n",
    "        # look for <tag=_tag, class=_class>\n",
    "        soup1 = soup.find(_tag, class_=_class)\n",
    "        # if such tag has been found, look then for <tag=_tag, class='number> where the value \n",
    "        # we're interested in is stored! otherwise append -99\n",
    "        if soup1:\n",
    "            soup2 = soup1.find(_tag, class_='number') \n",
    "            # if such tag has been found, append its value to the _list, otherwise append -99\n",
    "            if soup2:\n",
    "                _list.append({new_df_column_name: soup2.text})\n",
    "            else:\n",
    "                _list.append({new_df_column_name: -99})\n",
    "        else:\n",
    "            _list.append({new_df_column_name: -99})\n",
    "    # convert _list to dataframe and return it\n",
    "    return pd.DataFrame.from_dict(_list).replace({r'\\n': ''},\\\n",
    "                                                 regex=True).replace({r',': ''},\\\n",
    "                                                                     regex=True).apply(pd.to_numeric).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining HTML tag and class attributes that we want to find\n",
    "tofind = [{'tag':'div', 'class': 'total faculty'}, \n",
    "          {'tag':'div', 'class': 'inter faculty'}, \n",
    "          {'tag':'div', 'class': 'total student'}, \n",
    "          {'tag':'div', 'class': 'total inter'}]\n",
    "\n",
    "# creating DataFrame with the data found (NaN values = -99)\n",
    "details_df = pd.concat([my_find(tofind[0], 'fac_memb_tot', rank_df),\n",
    "                        my_find(tofind[1], 'fac_memb_int', rank_df),\n",
    "                        my_find(tofind[2], 'nb_stud_tot', rank_df),\n",
    "                        my_find(tofind[3], 'nb_stud_int', rank_df)], axis=1)\n",
    "\n",
    "# concatenate the DataFrames into a unique one\n",
    "details_df.set_index(rank_df.index, inplace=True)\n",
    "QS_df = pd.concat([rank_df, details_df], axis=1)\n",
    "\n",
    "# cleaning the unique DataFrame (deleting the = in rank_display)\n",
    "QS_df.drop(['url'], axis=1, inplace=True)\n",
    "QS_df.rank_display = QS_df.rank_display.replace({r'=': ''}, regex=True).apply(pd.to_numeric).astype(int)\n",
    "\n",
    "# Creating the faculty_members_ratio and number_of_students ratio\n",
    "QS_df['fac_memb_ratio'] = QS_df.fac_memb_tot / QS_df.nb_stud_tot\n",
    "QS_df['int_stud_ratio'] = QS_df.nb_stud_int / QS_df.nb_stud_tot\n",
    "\n",
    "# Deleting what's useless\n",
    "del details_df, tofind, rank_df, r, page_body, rank_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['core_id'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1088-9108f8b5c3c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# in order to prepare themerge with the corresponding TimesHigherEducation dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mQS_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mQS_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'core_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m QS_df.columns = ['QS_name', 'QS_rank', 'country', 'region', 'QS_fac_memb_tot', 'QS_fac_memb_int',\n\u001b[1;32m      6\u001b[0m                 'QS_nb_stud_tot', 'QS_nb_stud_int', 'QS_fac_memb_ratio', 'QS_int_stud_ratio']\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3624\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['core_id'] not contained in axis"
     ]
    }
   ],
   "source": [
    "# resetting the index, dropping the 'core_id' column wich is useless, and renaming the columns\n",
    "# in order to prepare themerge with the corresponding TimesHigherEducation dataframe\n",
    "QS_df.reset_index(inplace = True)\n",
    "QS_df.drop('core_id', axis=1, inplace = True)\n",
    "QS_df.columns = ['QS_name', 'QS_rank', 'country', 'region', 'QS_fac_memb_tot', 'QS_fac_memb_int',\n",
    "                'QS_nb_stud_tot', 'QS_nb_stud_int', 'QS_fac_memb_ratio', 'QS_int_stud_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMES HIGHER EDUCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = <Response [200]> // status_code = 200\n"
     ]
    }
   ],
   "source": [
    "# Making the request, beautifully soupping for extracting the dataframe\n",
    "URL = 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/'\\\n",
    "                +'world_university_rankings_2018_limit0_369a9045a203e176392b9fb8f8c1cb2a.json'\n",
    "r = requests.get(URL)\n",
    "print('r = {r} // status_code = {status}'.format(r=r,status=r.status_code))\n",
    "r.content\n",
    "soupp = BeautifulSoup(r.content,'html.parser')\n",
    "rank_json = json.loads(r.text)\n",
    "THE_df = pd.DataFrame()\n",
    "THE_df = THE_df.from_dict(rank_json['data']).head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns of our interest and changing their name\n",
    "THE_df = THE_df[['name', 'aliases', 'location', 'rank', 'stats_number_students', 'stats_student_staff_ratio', 'stats_pc_intl_students']]\n",
    "THE_df.columns=['THE_name', 'THE_aliases', 'THE_location', 'THE_rank','THE_nb_stud_tot', 'THE_stats_student_staff_ratio','THE_int_stud_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# still the usual, hard-to-understand cleaning of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE_int_stud_ratio\n",
    "THE_df['THE_int_stud_ratio'] = THE_df['THE_int_stud_ratio'].str.replace('%', '').astype('double')/100\n",
    "\n",
    "# THE_nb_stud_tot\n",
    "THE_df['THE_nb_stud_tot'] = THE_df['THE_nb_stud_tot'].str.replace(',', '').astype('int')\n",
    "\n",
    "# THE_fac_memb_int is a missing data, I add it just to state it clearly\n",
    "THE_df['THE_fac_memb_int'] = -99\n",
    "\n",
    "# THE_fac_memb_tot\n",
    "THE_df['THE_stats_student_staff_ratio']=THE_df['THE_stats_student_staff_ratio'].astype(float)\n",
    "THE_df['THE_fac_memb_tot'] = THE_df['THE_nb_stud_tot']/THE_df['THE_stats_student_staff_ratio']\n",
    "THE_df['THE_fac_memb_tot'] = THE_df['THE_fac_memb_tot'].astype(int)\n",
    "\n",
    "# THE_nb_stud_int\n",
    "THE_df['THE_nb_stud_int'] = THE_df['THE_nb_stud_tot']*THE_df['THE_int_stud_ratio']\n",
    "THE_df['THE_nb_stud_int'] = THE_df['THE_nb_stud_int'].astype(int)\n",
    "\n",
    "# THE_fac_memb_ratio\n",
    "THE_df['THE_fac_memb_ratio'] = 1/THE_df['THE_stats_student_staff_ratio']\n",
    "THE_df=THE_df.drop('THE_stats_student_staff_ratio', axis=1)\n",
    "\n",
    "# THE_rank\n",
    "THE_df['THE_rank'] = THE_df['THE_rank'].astype(str)\n",
    "THE_df['THE_rank'] = THE_df['THE_rank'].replace({r'=': ''}, regex=True).apply(pd.to_numeric).astype(int)\n",
    "\n",
    "# let's now have a look at the clean THE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY FUNCTIONS --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return dataframes df1,df2 with only the rows where index in column1 exist also in column2 and vice versa\n",
    "# and also a third dataframe with the non mergeable rows.\n",
    "\n",
    "def exclude_differences(df1,df2,column1,column2):\n",
    "    tmp1 = df1[column1].value_counts().index\n",
    "    tmp2 = df2[column2].value_counts().index\n",
    "    my_diff = list(set(tmp1).symmetric_difference(set(tmp2)))\n",
    "    out_1 = df1.copy()\n",
    "    out_2 = df2.copy()\n",
    "    out_3 = out_1.iloc[0:0,:].copy()\n",
    "    \n",
    "    for loc in my_diff:\n",
    "        # we wanna keep track of non meargeable elements\n",
    "        out_31 = out_1[out_1[column1] == loc] \n",
    "        out_32 = out_2[out_2[column2] == loc]\n",
    "        out_3 = pd.concat([out_3, out_31])\n",
    "        out_3 = pd.concat([out_3, out_32])\n",
    "        # now we get rid of these rows\n",
    "        out_1 = out_1[out_1[column1] != loc] \n",
    "        out_2 = out_2[out_2[column2] != loc]\n",
    "                \n",
    "    return out_1, out_2, out_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merging_by_name(THE,QS,prob_limit):\n",
    "    # creating two lists with the names of the universities from the two datasets\n",
    "    the = THE.copy()\n",
    "    qs = QS.copy()\n",
    "    THE_name = list(the.loc[:,'THE_name'])\n",
    "    QS_name = list(qs.loc[:,'QS_name'])\n",
    "\n",
    "    #initializing a new column of the THE_df with the corresponding QS name found by the matching function\n",
    "    # just to control that everything went smoothly\n",
    "    the['THE_corresponding QS name'] = 'unknown'\n",
    "    the['prob'] = 'unknown'\n",
    "\n",
    "    # MATCHING FUNCTION\n",
    "    # finding the probable corresponding name in the QS dataframe for each university\n",
    "    for i,THE_uni in enumerate(THE_name):\n",
    "        QS_uni, prob=process.extractOne(THE_uni, QS_name, scorer=fuzz.token_sort_ratio)\n",
    "        if prob>=prob_limit: #if prob<87, I observed that the algorithm matches diffeent universities!! 97 is a good limit\n",
    "            the.loc[(the['THE_name']== THE_uni) , \"THE_corresponding QS name\"] = QS_uni\n",
    "            the.loc[(the['THE_name']== THE_uni) , 'prob'] = prob\n",
    "                     \n",
    "    # MERGING        \n",
    "    Unique_df=pd.merge(the,qs, left_on='THE_corresponding QS name', right_on='QS_name', how = 'outer')\n",
    "    return Unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_unique_df(THE,QS,prob):\n",
    "    unique_list = []\n",
    "    for loc in QS['country'].value_counts().sort_index().index:\n",
    "        the= THE.loc[THE['THE_location'] == loc , :].copy()\n",
    "        qs  = QS.loc[QS['country'] == loc , :].copy()\n",
    "        unique = merging_by_name(the,qs,prob)\n",
    "        unique = unique.dropna(axis=0,how='any')\n",
    "        unique_list.append(unique)\n",
    "    unique_out = pd.concat(unique_list)\n",
    "    return unique_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_matched_uni(df,name, list_of_matched_uni):\n",
    "    out = df.copy()\n",
    "    for x in out[name]:\n",
    "        if x in list_of_matched_uni:\n",
    "            out.drop(out[out[name] == x].index[0], inplace=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_to_non_mergeable(non_mergeable,THE,QS,errors_THE,errors_QS):\n",
    "    out = non_mergeable.copy()\n",
    "\n",
    "    for x in errors_THE:\n",
    "        out = pd.concat([out,THE.loc[THE['THE_name'] == x]])\n",
    "    for x in errors_QS:\n",
    "        out = pd.concat([out,QS.loc[QS['QS_name'] == x]])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next two functions are for text transformation improve fuzz accuracy, we first provide examples of how the libraries work:\n",
    "\n",
    "fuzz influenced by:  äàö _ \n",
    "\n",
    "fuzz not influenced by:  éè'-()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malaga \n",
      " Ecole Normale Superieure de Lyon \n",
      " Scuola Superiore Sant'Anna Pisa di Studi Universitari e di Perfezionamento\n",
      "aeaaeou_euE\n",
      "Malaga\n"
     ]
    }
   ],
   "source": [
    "# example of use for the library unidecode\n",
    "\n",
    "accented_string = 'Málaga'\n",
    "# accented_string is of type 'unicode'\n",
    "unaccented_string = unidecode.unidecode(accented_string)\n",
    "unaccented_string2 = unidecode.unidecode(string_tmp)\n",
    "# unaccented_string contains 'Malaga'and is of type 'str'\n",
    "print(unaccented_string,'\\n',unaccented_string2,'\\n',string_tmp2 )\n",
    "print(unidecode.unidecode('äèàäéöû_èüÈ'))\n",
    "print(unidecode.unidecode(accented_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 foe bar\n",
      "paris sorbonne university  paris 4\n",
      "scuola superiore sant anna pisa di studi universitari e di perfezionamento\n",
      "eo\n"
     ]
    }
   ],
   "source": [
    "# example of use for the collections library\n",
    "\n",
    "table = collections.defaultdict(lambda: None)\n",
    "table.update({\n",
    "    ord('é'):'e',\n",
    "    ord('ô'):'o',\n",
    "    ord('ö'):'o',\n",
    "    ord(' '):' ',\n",
    "    ord('\\''):' ',\n",
    "    ord('-'): ' ',\n",
    "    ord('\\N{NO-BREAK SPACE}'): ' ',\n",
    "    ord('\\N{EN SPACE}'): ' ',\n",
    "    ord('\\N{EM SPACE}'): ' ',\n",
    "    ord('\\N{THREE-PER-EM SPACE}'): ' ',\n",
    "    ord('\\N{FOUR-PER-EM SPACE}'): ' ',\n",
    "    ord('\\N{SIX-PER-EM SPACE}'): ' ',\n",
    "    ord('\\N{FIGURE SPACE}'): ' ',\n",
    "    ord('\\N{PUNCTUATION SPACE}'): ' ',\n",
    "    ord('\\N{THIN SPACE}'): ' ',\n",
    "    ord('\\N{HAIR SPACE}'): ' ',\n",
    "    ord('\\N{ZERO WIDTH SPACE}'): ' ',\n",
    "    ord('\\N{NARROW NO-BREAK SPACE}'): ' ',\n",
    "    ord('\\N{MEDIUM MATHEMATICAL SPACE}'): ' ',\n",
    "    ord('\\N{IDEOGRAPHIC SPACE}'): ' ',\n",
    "    ord('\\N{IDEOGRAPHIC HALF FILL SPACE}'): ' ',\n",
    "    ord('\\N{ZERO WIDTH NO-BREAK SPACE}'): ' ',\n",
    "    ord('\\N{TAG SPACE}'): ' ',\n",
    "    })\n",
    "table.update(dict(zip(map(ord,string.ascii_uppercase), string.ascii_lowercase)))\n",
    "table.update(dict(zip(map(ord,string.ascii_lowercase), string.ascii_lowercase)))\n",
    "table.update(dict(zip(map(ord,string.digits), string.digits)))\n",
    "\n",
    "print('123 fôé BAR҉'.translate(table,))\n",
    "print('Paris-Sorbonne University – Paris 4'.translate(table,))\n",
    "print(string_tmp2.translate(table,))\n",
    "print('éöàäèüÒàèò'.translate(table,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_accent_in_name(df,name):\n",
    "    out=df#.copy()\n",
    "    for x in out[name]:\n",
    "        x = unidecode.unidecode(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_in_name(df,name):\n",
    "    out=df.copy()\n",
    "    for x in out[name]:\n",
    "        x = x.translate(table,)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Those where our dataset, from now we try to merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there is not the same set of countries in the two ranking. The universities that come from a country not present in the other dataset are thus considered not-mergeable. Note the we replaced \"Russian federation \" by \"Russia\" to have a match, assuming that they are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my set of country present in only one of the rankings:\n",
      " ['Malaysia', 'Argentina', 'Chile', 'Brazil', 'India', 'Luxembourg', 'Israel', 'Saudi Arabia', 'Mexico']\n"
     ]
    }
   ],
   "source": [
    "# Set russian federation equal russia for comparing countries\n",
    "THE_df.loc[THE_df['THE_location']=='Russian Federation','THE_location']='Russia'\n",
    "\n",
    "# find differences in listed countries\n",
    "tmp1 = QS_df['country'].value_counts().index\n",
    "tmp2 = THE_df['THE_location'].value_counts().index\n",
    "my_diff = list(set(tmp1).symmetric_difference(set(tmp2)))\n",
    "print('Here is my set of country present in only one of the rankings:\\n',my_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 remove unis of countries that are not in both rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QS_df remaining rows:  187     THE_df remaining rows:  199\n",
      "Rows in non-mergeable:  14\n"
     ]
    }
   ],
   "source": [
    "QS_step1, THE_step1, non_mergeable = exclude_differences(QS_df,THE_df,'country','THE_location')\n",
    "\n",
    "print('QS_df remaining rows: ',QS_step1.shape[0],'    THE_df remaining rows: ',THE_step1.shape[0])\n",
    "print('Rows in non-mergeable: ',non_mergeable.shape[0])\n",
    "QS_step1.sort_values('country',inplace=True)\n",
    "THE_step1.sort_values('THE_location',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that non_meargeable contains only a subset of all the non-meargeables rows, we will append rows to it as the analysis continues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 take out 100% fitting unis names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged rows at this step:  107\n",
      "Non mergeable dataframe #rows:  16\n",
      "THE before:  199    THE after:  92\n",
      "QS before:  187    QS after:  80\n"
     ]
    }
   ],
   "source": [
    "Unique_df = create_unique_df(THE_step1,QS_step1,100)\n",
    "\n",
    "# list of matched unis by ranking\n",
    "QS_matched_uni = list(Unique_df['QS_name'])\n",
    "THE_matched_uni = list(Unique_df['THE_name'])\n",
    "\n",
    "# update the remaining ranking dataframes\n",
    "THE_step2 = drop_matched_uni(THE_step1,'THE_name',THE_matched_uni)\n",
    "QS_step2 = drop_matched_uni(QS_step1,'QS_name', QS_matched_uni)\n",
    "\n",
    "# print to check that the dataframes have the correct size:\n",
    "print('merged rows at this step: ',Unique_df.shape[0])\n",
    "print('Non mergeable dataframe #rows: ',non_mergeable.shape[0])\n",
    "print('THE before: ',THE_step1.shape[0],'   THE after: ',THE_step2.shape[0])\n",
    "print('QS before: ',QS_step1.shape[0],'   QS after: ',QS_step2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 go to 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged rows at this step:  37\n",
      "Non mergeable dataframe #rows:  16\n",
      "THE before:  92    THE after:  55\n",
      "QS before:  80    QS after:  43\n"
     ]
    }
   ],
   "source": [
    "THE_step2_tmp = remove_accent_in_name(THE_step2,'THE_name') # try to remove accents\n",
    "QS_step2_tmp = remove_accent_in_name(QS_step2,'QS_name') # but without succes\n",
    "\n",
    "Unique_df2 = create_unique_df(THE_step2_tmp,QS_step2_tmp,90) #90 since 100 gave nothing\n",
    "\n",
    "# list of matched unis by ranking (at this step)\n",
    "THE_matched_uni = list(Unique_df2['THE_name'])\n",
    "QS_matched_uni = list(Unique_df2['QS_name'])\n",
    "\n",
    "# update the remaining ranking dataframes\n",
    "THE_step3 = drop_matched_uni(THE_step2,'THE_name',THE_matched_uni)\n",
    "QS_step3 = drop_matched_uni(QS_step2,'QS_name', QS_matched_uni)\n",
    "\n",
    "# usual cross check\n",
    "print('merged rows at this step: ',Unique_df2.shape[0])\n",
    "print('Non mergeable dataframe #rows: ',non_mergeable.shape[0])\n",
    "print('THE before: ',THE_step2.shape[0],'   THE after: ',THE_step3.shape[0])\n",
    "print('QS before: ',QS_step2.shape[0],'   QS after: ',QS_step3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 go to 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged rows at this step:  8\n",
      "Non mergeable dataframe #rows:  16\n",
      "THE before:  55    THE after:  46\n",
      "QS before:  43    QS after:  34\n"
     ]
    }
   ],
   "source": [
    "THE_step3_tmp = remove_characters_in_name(THE_step3,'THE_name') # tried to remove special characters\n",
    "QS_step3_tmp = remove_characters_in_name(QS_step3,'QS_name') # doesn't work though\n",
    "\n",
    "Unique_df3 = create_unique_df(THE_step3_tmp,QS_step3_tmp,80) \n",
    "\n",
    "# errors made\n",
    "errors = [\"Trinity College Dublin\",'University College Dublin']\n",
    "\n",
    "# update the non-mergeable dataframe\n",
    "non_mergeable = pd.concat([non_mergeable,THE_step3.loc[THE_step3['THE_name'] == \"Trinity College Dublin\"]]) \n",
    "non_mergeable = pd.concat([non_mergeable,QS_step3.loc[QS_step3['QS_name'] == \"University College Dublin\"]]) \n",
    "\n",
    "Unique_df3 = Unique_df3.loc[Unique_df3['THE_name'] != \"Trinity College Dublin\"] #the only one with an error\n",
    "\n",
    "# list of matched unis by ranking (at this step)\n",
    "THE_matched_uni = list(Unique_df3['THE_name'])  + errors\n",
    "QS_matched_uni = list(Unique_df3['QS_name']) + errors\n",
    "\n",
    "# update the remaining ranking dataframes\n",
    "THE_step4 = drop_matched_uni(THE_step3,'THE_name',THE_matched_uni)\n",
    "QS_step4 = drop_matched_uni(QS_step3,'QS_name', QS_matched_uni)\n",
    "\n",
    "# remove duplicates ()usefull mainly if we runs multiples times this cell\n",
    "non_mergeable = non_mergeable.drop_duplicates()\n",
    "\n",
    "# usual cross check \n",
    "print('merged rows at this step: ',Unique_df3.shape[0])\n",
    "print('Non mergeable dataframe #rows: ',non_mergeable.shape[0])\n",
    "print('THE before: ',THE_step3.shape[0],'   THE after: ',THE_step4.shape[0])\n",
    "print('QS before: ',QS_step3.shape[0],'   QS after: ',QS_step4.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 go to 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged rows at this step:  3\n",
      "Non mergeable dataframe #rows:  22\n",
      "THE before:  46    THE after:  39\n",
      "QS before:  34    QS after:  29\n"
     ]
    }
   ],
   "source": [
    "# df created with the matching function, but some errors are present\n",
    "Unique_df4 = create_unique_df(THE_step4,QS_step4,70) \n",
    "\n",
    "# errors at this step\n",
    "errors = ['University of Sussex','University of Reading',\\\n",
    "          'University of East Anglia','University of Bath',\\\n",
    "          'University of Leicester','University of Dundee']\n",
    "\n",
    "# errors by ranking\n",
    "errors_THE = ['University of Sussex','University of East Anglia','University of Dundee','University of Leicester']\n",
    "errors_QS = ['University of Reading','University of Bath']\n",
    "\n",
    "# clean the unique_df from its errors\n",
    "for x in Unique_df4['QS_name']:\n",
    "    if x in errors:\n",
    "        Unique_df4 = Unique_df4[Unique_df4['QS_name'] != x]\n",
    "for x in Unique_df4['THE_name']:\n",
    "    if x in errors:\n",
    "        Unique_df4 = Unique_df4[Unique_df4['THE_name'] != x]\n",
    "\n",
    "#update non mergeable df\n",
    "non_mergeable = append_to_non_mergeable(non_mergeable,THE_step4,QS_step4,\\\n",
    "                                                    errors_THE,errors_QS)\n",
    "#drop duplicates\n",
    "non_mergeable = non_mergeable.drop_duplicates()\n",
    "\n",
    "# list of matched unis by ranking (at this step)\n",
    "THE_matched_uni = list(Unique_df4['THE_name'])  + errors_THE\n",
    "QS_matched_uni = list(Unique_df4['QS_name']) + errors_QS\n",
    "\n",
    "# update the remaining ranking dataframes\n",
    "THE_step5 = drop_matched_uni(THE_step4,'THE_name',THE_matched_uni)\n",
    "QS_step5 = drop_matched_uni(QS_step4,'QS_name', QS_matched_uni)\n",
    "\n",
    "# usual cross check \n",
    "print('merged rows at this step: ',Unique_df4.shape[0])\n",
    "print('Non mergeable dataframe #rows: ',non_mergeable.shape[0])\n",
    "print('THE before: ',THE_step4.shape[0],'   THE after: ',THE_step5.shape[0])\n",
    "print('QS before: ',QS_step4.shape[0],'   QS after: ',QS_step5.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final STEP all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged rows at this step:  4\n",
      "Non mergeable dataframe #rows:  82\n",
      "THE before:  39    THE after:  35\n",
      "QS before:  29    QS after:  25\n"
     ]
    }
   ],
   "source": [
    "# create merged dataframe with matching function. But some errors are present\n",
    "Unique_df5 = create_unique_df(THE_step5,QS_step5,10)\n",
    "\n",
    "# correct matchings for THE_ranking (since they are unique due to our merging function)\n",
    "rights = ['University of Tübingen','LMU Munich','University of Freiburg','Scuola Superiore Sant’Anna']\n",
    "\n",
    "# errors made: (incorrect matching)\n",
    "errors = list(set(rights).symmetric_difference(set(list(Unique_df5['THE_name']))))\n",
    "\n",
    "for x in errors:\n",
    "    Unique_df5 = Unique_df5[Unique_df5['THE_name'] != x] \n",
    "\n",
    "\n",
    "# list of matched unis by ranking (at this step)\n",
    "THE_matched_uni = list(Unique_df5['THE_name']) \n",
    "QS_matched_uni = list(Unique_df5['QS_name']) \n",
    "\n",
    "# update the remaining ranking dataframes\n",
    "THE_step6 = drop_matched_uni(THE_step5,'THE_name',THE_matched_uni)\n",
    "QS_step6 = drop_matched_uni(QS_step5,'QS_name', QS_matched_uni)\n",
    "\n",
    "# update non_mergeables unis dataframe\n",
    "non_mergeable = pd.concat([non_mergeable,THE_step6,QS_step6])\n",
    "non_mergeable = non_mergeable.drop_duplicates()\n",
    "\n",
    "# usual cross check \n",
    "print('merged rows at this step: ',Unique_df5.shape[0])\n",
    "print('Non mergeable dataframe #rows: ',non_mergeable.shape[0])\n",
    "print('THE before: ',THE_step5.shape[0],'   THE after: ',THE_step6.shape[0])\n",
    "print('QS before: ',QS_step5.shape[0],'   QS after: ',QS_step6.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged:  159   non-merged 82 \n",
      "missing 0\n",
      "\n",
      "if 0 missing, it is probable the merge is successfull\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([Unique_df,Unique_df2,Unique_df3,Unique_df4,Unique_df5])\n",
    "num_merged = merged_df.shape[0]\n",
    "\n",
    "non_mergeable = non_mergeable.drop_duplicates()\n",
    "num_non_merged = non_mergeable.shape[0]\n",
    "print ('merged: ',num_merged,'  non-merged',num_non_merged,'\\nmissing',(200-num_merged)*2-num_non_merged)\n",
    "print('\\nif 0 missing, it is probable the merge is successfull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
