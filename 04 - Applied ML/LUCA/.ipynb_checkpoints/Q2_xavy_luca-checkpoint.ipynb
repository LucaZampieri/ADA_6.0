{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load dataset and vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier of news is going to be built to assign 20 news categories. Let's import the useful libraries and the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import itertools\n",
    "from pandas.io.json import json_normalize\n",
    "from pathlib import Path \n",
    "import os \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')#, remove = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Show categories list\n",
    "pprint(list(newsgroups.target_names))\n",
    "classes = newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset shape and how is like each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filenames shape: (18846,)\n",
      "Target shape: (18846,) \n",
      "\n",
      "The categories of newsgroups are:  ['DESCR', 'data', 'description', 'filenames', 'target', 'target_names'] \n",
      "\n",
      "description is:  the 20 newsgroups by date dataset\n",
      "filenames contains the location of the files in the hardware running the code \n"
     ]
    }
   ],
   "source": [
    "# The real data lies in the filenames and target attributes(target category)\n",
    "print('Filenames shape:', newsgroups.filenames.shape)\n",
    "print('Target shape:',newsgroups.target.shape,'\\n')\n",
    "\n",
    "# Show \n",
    "print('The categories of newsgroups are: ',list(pd.DataFrame.from_dict(json_normalize(newsgroups), orient='columns').columns),'\\n')\n",
    "print('description is: ',newsgroups.description)\n",
    "print('filenames contains the location of the files in the hardware running the code ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is vectorized. The following condierations have been taken in account:\n",
    "* Small `max_feature` is faster and reduces a bit the chances of overfitting\n",
    "* If word in `max_d` portion of the files, then it get ignored\n",
    "* Strip eventual accents (even though we should not have problems with english)\n",
    "* For this current version we can only eliminate \"english\" key word, even though it should already be taken by max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the text dataset\n",
    "vectorizer = TfidfVectorizer(max_features=10000,\\\n",
    "                             max_df=0.7,\\\n",
    "                             strip_accents = 'ascii',\n",
    "                             stop_words='english')\n",
    "vectors = vectorizer.fit_transform(newsgroups.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in training, testing and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the predictors matrix and the predicted variable are created. Then the split is done as follows:\n",
    "- Training: 80%\n",
    "- Testing: 10%\n",
    "- Validation: 10% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create predictors and predicted variable\n",
    "X = vectors\n",
    "y = newsgroups.target\n",
    "\n",
    "# Split training, testing and validation tests\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.8)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, random_state=0, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier, in this case the Random Forest classifier can be created and fitted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the different news categories, a Random Forest classifier is choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=50, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the seed used in this homework\n",
    "seed = 50\n",
    "\n",
    "# Create a fit the classifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To print with different fonts\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInitial score:\u001b[0m\n",
      "Train score: 1.0\n",
      "Test score: 0.830769230769\n",
      "Validation score: \u001b[91m0.82599469496\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# First Evaluation\n",
    "print (color.BOLD + 'Initial score:' + color.END)\n",
    "print('Train score:',rfc.score(X_train, y_train))\n",
    "print('Test score:',rfc.score(X_test, y_test))\n",
    "print('Validation score:',color.RED+str(rfc.score(X_val, y_val))+color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there is a huge difference between the train score and the test and validation scores. This is mainly due to the overfit of the classifier on the training set.\n",
    "\n",
    "Let's investigate the main attributes of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10000\n",
      "Features importances: [  5.74276830e-04   2.07053299e-04   1.25403332e-04 ...,   1.30413849e-06\n",
      "   2.50585632e-04   8.65350439e-06]\n",
      "Maximal importance of a feature: 0.0101609887333\n",
      "\n",
      "Depths: \n",
      " count    100.00000\n",
      "mean     305.48000\n",
      "std       35.92008\n",
      "min      223.00000\n",
      "25%      278.50000\n",
      "50%      304.50000\n",
      "75%      329.25000\n",
      "max      385.00000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print main attributes of the classifier\n",
    "print('Number of features:',rfc.n_features_)\n",
    "print('Features importances:',rfc.feature_importances_)\n",
    "print('Maximal importance of a feature:',max(rfc.feature_importances_))\n",
    "depths = pd.Series([estimator.tree_.max_depth for estimator in rfc.estimators_])\n",
    "print ('\\nDepths: \\n',depths.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the maximal feature importance is a 1% of all features importances. This means that our data can be badly explained in the different dimensions. In a further analysis, PCA could be implemented in order to increase the feature importance and reduce the dimensionality. \n",
    "\n",
    "Watching at the max depth statistical description of all estimators will help us to know how to tune better this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Grid search for finding better parameters\n",
    "In order to find the best possible parameters for our classifier, a grid search on the number of estiamotors and the maximum depth of each estimator. \n",
    "\n",
    "**NOTE**: Since the computation are long, we are gonna save the results in pickle files. We used them to write to a binary file the content of a given variable, in our case it will be the content of grid search. The goal being to not recompute the results each time. We saved all thoses _.pickle_ files in the subdirectory **pickles/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'pickles/'\n",
    "\n",
    "# function to save results in a pickle file\n",
    "def save_results(var_name, file_name):\n",
    "    file_path = DATA_FOLDER + file_name +'.pickle'\n",
    "    my_file = Path(file_path)\n",
    "    my_dir = Path(DATA_FOLDER)\n",
    "    if not(my_dir.is_dir()):\n",
    "        os.makedirs(DATA_FOLDER)\n",
    "        save_results(var_name, file_name)\n",
    "    elif my_file.is_file():\n",
    "        print('WARNING! This filename already exusted so we wrote in \"overrided.pickle\" PLEASE MANUALLY CHANGE THE NAME'+\\\n",
    "             '\\n CHancge the file name with your name')\n",
    "        with open(DATA_FOLDER + 'overrided.pickle', 'wb') as file:\n",
    "            pickle.dump(var_name, file)\n",
    "    else:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(var_name, file)\n",
    "\n",
    "# Function to read the pickles file\n",
    "def read_pickle(file_name):\n",
    "    file_path = DATA_FOLDER + str(file_name) + '.pickle'\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        out = pickle.load(file)\n",
    "    return out\n",
    "\n",
    "# function to prin the scores\n",
    "def print_scores(grid_search_var):\n",
    "    print('Train score:',grid_search_var.score(X_train, y_train))\n",
    "    print('Test score:',grid_search_var.score(X_test, y_test))\n",
    "    print('Validation score:',color.RED+str(grid_search_var.score(X_val, y_val))+color.END)\n",
    "    \n",
    "# since in the following we will change the max number of features, here is a function that does that\n",
    "def change_max_features_number(max_features,max_df=0.7):\n",
    "    newsgroups = fetch_20newsgroups(subset='all')\n",
    "    classes = newsgroups.target_names\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features,\\\n",
    "                                 max_df=max_df,\\\n",
    "                                 strip_accents = 'ascii',\n",
    "                                 stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(newsgroups.data)\n",
    "    # Create predictors and predicted variable\n",
    "    X = vectors\n",
    "    y = newsgroups.target\n",
    "\n",
    "    # Split training, testing and validation tests\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.8) # 80% train\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, random_state=0, train_size=0.5) # 10% test, 10% validation\n",
    "    return X_train, y_train, X_test, X_val, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computation cost reasons, this code has been runned in different computers. The code used to grid-search on the parameters is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Parameters of the grid search\n",
    "n_estim_range = [200, 400, 700]\n",
    "max_depth_range = [50, 100, 200, 500, 600, 900]\n",
    "\n",
    "paramgrid = {'n_estimators': n_estim_range,'max_depth': max_depth_range}\n",
    "\n",
    "# Grid search on the number of estimators\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=seed), paramgrid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters:',grid_search.best_params_)\n",
    "\n",
    "# Evaluation\n",
    "print('Train score:',grid_search.score(X_train, y_train))\n",
    "print('Test score:',grid_search.score(X_test, y_test))\n",
    "print('Validation score:',grid_search.score(X_val, y_val))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save the model parameters in a pickle file, the following code has been used:\n",
    ">``` python\n",
    "save_results(grid_search, 'removing_features_1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1.1 First approach: Run removing headers, footers and quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been imported removing headers, footers and quotes. Let's investigate if that can influence on a bad fitting of our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all',remove = ('headers', 'footers', 'quotes'))\n",
    "classes = newsgroups.target_names\n",
    "vectorizer = TfidfVectorizer(max_features=10000,\\\n",
    "                             max_df=0.7,\\\n",
    "                             strip_accents = 'ascii',\n",
    "                             stop_words='english')\n",
    "vectors = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Create predictors and predicted variable\n",
    "X = vectors\n",
    "y = newsgroups.target\n",
    "\n",
    "# Split training, testing and validation tests\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.8) # 80% train\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, random_state=0, train_size=0.5) # 10% test, 10% validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with \"remove = (\"headers\", \"footers\", \"quotes\")\" and with the following set of parameters:\n",
      "n_estim_range = [200, 400, 700]\n",
      "max_depth_range = [50, 100, 200, 500, 600, 900]\n",
      "\n",
      "Optimal features found:  {'max_depth': 500, 'n_estimators': 400}\n",
      "which gave the following results:\n",
      "Train score: 0.971875829132\n",
      "Test score: 0.663129973475\n",
      "Validation score: \u001b[91m0.640848806366\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "removed_features_GS = read_pickle('removing_features_1')\n",
    "\n",
    "print('Run with \"remove = (\"headers\", \"footers\", \"quotes\")\" and with the following set of parameters:\\n'+\\\n",
    "      'n_estim_range = [200, 400, 700]\\n'+\\\n",
    "      'max_depth_range = [50, 100, 200, 500, 600, 900]\\n')\n",
    "\n",
    "print('Optimal features found: ',removed_features_GS.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(removed_features_GS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** We can see that removing the headers, footers and quotes decreases our accuracy of preditcion, even if it is in some sense a more thruthfull classification since only the content is analysed. <br />\n",
    "**Thus in the rest of the analysis we thus not remove the features, but we should keep in mind that consideration**<br />\n",
    "So we recompute the training and testss sets withour removing the headers, footers and quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the grid search:\n",
    "a first run (we cannot reload the pickle file for incompatibilities here so we hard-coded the results) for the other we will be able to load the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with without removing anything and with the following set of parameters:\n",
      "n_estim_range = [ 316  464  681 1000]\n",
      "max_depth_range = [ 1000  2154  4641 10000]\n",
      "\n",
      "We found:\n",
      "best n_estimators = 1000\n",
      "best max_depth = 1000\n",
      "train score = 1.0\n",
      "test_score = 0.841909814324\n",
      "val_score = 0.835013262599\n"
     ]
    }
   ],
   "source": [
    "#nightly_run_1 = read_pickle('gilcompa')\n",
    "print('Run with without removing anything and with the following set of parameters:\\n'+\\\n",
    "      'n_estim_range = [ 316  464  681 1000]\\n'+\\\n",
    "      'max_depth_range = [ 1000  2154  4641 10000]\\n')\n",
    "\n",
    "print('We found:\\n'+\\\n",
    "      'best n_estimators = 1000\\n'+\\\n",
    "     'best max_depth = 1000\\n'+\\\n",
    "     'train score = 1.0\\n'+\\\n",
    "     'test_score = 0.841909814324\\n'+\\\n",
    "     'val_score = 0.835013262599')\n",
    "\n",
    "#print('Optimal features found: ',nightly_run_1.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** It seems that the bigger the _n_estimators_ the better. On the other hand the max_depth seems not to be reached. The overfitting (1.0 on the training set) is inacceptable.\n",
    "\n",
    "We thus reduce both ranges a do another run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with \"remove = (\"headers\", \"footers\", \"quotes\")\" and with the following set of parameters:\n",
      "max_depth_range = [300, 500, 700, 900]\n",
      "n_estim_range = [200, 500, 900]\n",
      "\n",
      "Optimal features found:  {'max_depth': 300, 'n_estimators': 900}\n",
      "which gave the following results:\n",
      "Train score: 1.0\n",
      "Test score: 0.841379310345\n",
      "Validation score: 0.835543766578\n"
     ]
    }
   ],
   "source": [
    "nightly_run_2 = read_pickle('martino')\n",
    "print('Run with the following set of parameters:\\n'+\\\n",
    "      'max_depth_range = [300, 500, 700, 900]\\n'+\\\n",
    "      'n_estim_range = [200, 500, 900]\\n')\n",
    "print('Optimal features found: ',nightly_run_2.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(nightly_run_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Slightly better results have been obtained with smaller parameters. However, the strong overfitting is still present. We may then change approach and reduce the maximum number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced number of features\n",
    "\n",
    "we freeze the number of estimators and the max depth to 400 to reduce the computational cost of this operation. We realize that this is far from optimal but it is just to have a better understanding of how it works.<br>\n",
    "Since we will recompute often the sets changing the number of features we define the following function: \n",
    "\n",
    "We recompute training and tests with 5000 max_features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used fixed depth and estimators for this quick analysis: \n",
      "max_depth = 400, n_estimators = 400}\n",
      "\n",
      "Optimal features found:  {'max_depth': 400, 'n_estimators': 400}\n",
      "which gave the following results:\n",
      "Train score: 0.999933669408\n",
      "Test score: 0.826525198939\n",
      "Validation score: \u001b[91m0.823342175066\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reduced_5000 = read_pickle('5000_max_features')\n",
    "print('We used fixed depth and estimators for this quick analysis: \\n'+\"max_depth = 400, n_estimators = 400}\\n\")\n",
    "print('Optimal features found: ',reduced_5000.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(reduced_5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** We finnaly don't get 1.0 as training but we are still very close to one... But lets see what happens.\n",
    "\n",
    "We recompute training and tests with 7000 max_features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used fixed depth and estimators for this quick analysis: \n",
      "max_depth = 400, n_estimators = 400}\n",
      "\n",
      "Optimal features found:  {'max_depth': 400, 'n_estimators': 400}\n",
      "which gave the following results:\n",
      "Train score: 0.999933669408\n",
      "Test score: 0.833421750663\n",
      "Validation score: \u001b[91m0.828116710875\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reduced_7000 = read_pickle('7000_max_features')\n",
    "print('We used fixed depth and estimators for this quick analysis: \\n'+\"max_depth = 400, n_estimators = 400}\\n\")\n",
    "print('Optimal features found: ',reduced_7000.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(reduced_7000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** the validation score increased from 0.8233 to 0.8281 which is a small inproovement.\n",
    "\n",
    "Lets go back to 10000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used fixed depth and estimators for this quick analysis: \n",
      "max_depth = 400, n_estimators = 400}\n",
      "\n",
      "Optimal features found:  {'max_depth': 400, 'n_estimators': 400}\n",
      "which gave the following results:\n",
      "Train score: 1.0\n",
      "Test score: 0.836604774536\n",
      "Validation score: \u001b[91m0.836074270557\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reduced_10000 = read_pickle('10000_max_features')\n",
    "print('We used fixed depth and estimators for this quick analysis: \\n'+\"max_depth = 400, n_estimators = 400}\\n\")\n",
    "print('Optimal features found: ',reduced_10000.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(reduced_10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** It gets a bit better again so we will continue... (even though we are again at 1.0 on the training which is really bad!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used fixed depth and estimators for this quick analysis: \n",
      "max_depth = 400, n_estimators = 400}\n",
      "\n",
      "Optimal features found:  {'max_depth': 400, 'n_estimators': 400}\n",
      "which gave the following results:\n",
      "Train score: 1.0\n",
      "Test score: 0.860477453581\n",
      "Validation score: \u001b[91m0.856233421751\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reduced_60000 = read_pickle('50000_max_features') # yes, the name is wrong haha\n",
    "print('We used fixed depth and estimators for this quick analysis: \\n'+\"max_depth = 400, n_estimators = 400}\\n\")\n",
    "print('Optimal features found: ',reduced_60000.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(reduced_60000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** We see that it keeps improoving even if there is that huge training score (that may be a bit problematic for the grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can now see the influence of max_df:\n",
    "We compute the sets with 10000 max_features and 0.8 of max_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(10000,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used fixed depth and estimators for this quick analysis: \n",
      "max_depth = 400, n_estimators = 400}\n",
      "\n",
      "Optimal features found:  {'max_depth': 400, 'n_estimators': 400}\n",
      "which gave the following results:\n",
      "Train score: 1.0\n",
      "Test score: 0.836604774536\n",
      "Validation score: \u001b[91m0.836074270557\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reduced_10000_08_max_df = read_pickle('08_max_df_10000_max_features')\n",
    "print('We used fixed depth and estimators for this quick analysis: \\n'+\"max_depth = 400, n_estimators = 400}\\n\")\n",
    "print('Optimal features found: ',reduced_10000_08_max_df.best_params_)\n",
    "print('which gave the following results:')\n",
    "# Evaluation\n",
    "print_scores(reduced_10000_08_max_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** which doesn't change anything, so we will let it at 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial conclusion:\n",
    "The bes tup to now is the one with a lot of max features and with a large number of estimators. Unfortunately we don't have the time to increase those parameters so we will be satisfied with **0.856233421751** as validation score.\n",
    "On the contrary, the max depth has to be kept lower because otherwise the overfitting will be too important. The lower the features number the lower should the depth of the tree be to avoid excessive overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the template found here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "\n",
    "we plotted the confusion matrix for one of the best cases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# give the right dimensions to the sets\n",
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced_60000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cf760073ceca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduced_60000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reduced_60000' is not defined"
     ]
    }
   ],
   "source": [
    "best_run = reduced_60000\n",
    "y_pred = best_run.predict(X_val)\n",
    "cm = confusion_matrix(y_pred, y_val)\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Greys);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is confusion on similar subjets like **soc.religion.christian** with **talk.religion.misc**, **talk.politics.misc** with **talk.politics.guns** and so on.\n",
    "We could use different methods on those closes subjects to further improve the accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.Results discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=50, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give the right dimensions to the sets\n",
    "X_train, y_train, X_test, X_val, y_test, y_val = change_max_features_number(60000) # run if needed\n",
    "best_run.estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 60000\n",
      "Features importances: [ 0.00059091  0.00030749  0.         ...,  0.          0.          0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    100.00000\n",
       "mean     305.48000\n",
       "std       35.92008\n",
       "min      223.00000\n",
       "25%      278.50000\n",
       "50%      304.50000\n",
       "75%      329.25000\n",
       "max      385.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of features:',best_run.estimator.n_features_)\n",
    "print('Features importances:',best_run.estimator.feature_importances_)\n",
    "depths = pd.Series([estimator.tree_.max_depth for estimator in rfc.estimators_])\n",
    "depths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e8f41e220ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_run' is not defined"
     ]
    }
   ],
   "source": [
    "np.sort(best_run.estimator.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=50, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
