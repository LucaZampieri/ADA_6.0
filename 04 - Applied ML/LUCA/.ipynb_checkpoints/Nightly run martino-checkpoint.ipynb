{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load dataset and vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier of news is going to be built to assign 20 news categories. Let's import the dataset and check the different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups # Data function that dowloads the data from archive\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pathlib import Path # to check if file exists\n",
    "import os # get current directory\n",
    "import pickle\n",
    "\n",
    "\n",
    "# imported because of the splitting of the questions:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')#, remove = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Show categories list\n",
    "pprint(list(newsgroups.target_names))\n",
    "classes = newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset shape and how is like each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filenames shape: (18846,)\n",
      "Target shape: (18846,) \n",
      "\n",
      "The categories of newsgroups are:  ['DESCR', 'data', 'description', 'filenames', 'target', 'target_names'] \n",
      "\n",
      "description is:  the 20 newsgroups by date dataset \n",
      "\n",
      "filenames contains the location of the files in the hardware running the code  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# The real data lies in the filenames and target attributes(target category)\n",
    "print('Filenames shape:', newsgroups.filenames.shape)\n",
    "print('Target shape:',newsgroups.target.shape,'\\n')\n",
    "\n",
    "# Show \n",
    "print('The categories of newsgroups are: ',list(pd.DataFrame.from_dict(json_normalize(newsgroups), orient='columns').columns),'\\n')\n",
    "print('description is: ',newsgroups.description,'\\n')\n",
    "print('filenames contains the location of the files in the hardware running the code ','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 10000)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the text dataset\n",
    "\n",
    "# small is faster and reduces a bit the chances of overfitting\n",
    "# if word in max_df portion of the files then ignore\n",
    "# strip eventual accents (even though we should not have problems with english)\n",
    "# for this current version we can only eliminate \"english\" key word, even though it should already be taken by max_df!!\n",
    "vectorizer = TfidfVectorizer(max_features=10000,\\\n",
    "                             max_df=0.7,\\\n",
    "                             strip_accents = 'ascii',\n",
    "                             stop_words='english')\n",
    "vectors = vectorizer.fit_transform(newsgroups.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in training, testing and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training (80%), testing(10%) and validation (10%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zampieri/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create predictors and predicted variable\n",
    "X = vectors\n",
    "y = newsgroups.target\n",
    "\n",
    "# Split training, testing and validation tests\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.8) # 80% train\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, random_state=0, train_size=0.5) # 10% test, 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=50, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_state = 50\n",
    "\n",
    "# Create a fit the classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=rand_state)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.830769230769\n",
      "Validation score: 0.82599469496\n"
     ]
    }
   ],
   "source": [
    "# First Evaluation\n",
    "print('Train score:',rfc.score(X_train, y_train))\n",
    "print('Test score:',rfc.score(X_test, y_test))\n",
    "print('Validation score:',rfc.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the first fitting of the classifier on the test and validation sets are not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10000\n",
      "Features importances: [  5.74276830e-04   2.07053299e-04   1.25403332e-04 ...,   1.30413849e-06\n",
      "   2.50585632e-04   8.65350439e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    100.00000\n",
       "mean     305.48000\n",
       "std       35.92008\n",
       "min      223.00000\n",
       "25%      278.50000\n",
       "50%      304.50000\n",
       "75%      329.25000\n",
       "max      385.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of features:',rfc.n_features_)\n",
    "print('Features importances:',rfc.feature_importances_)\n",
    "depths = pd.Series([estimator.tree_.max_depth for estimator in rfc.estimators_])\n",
    "depths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watching at the max depth statistical description all estimators will help us to know how to tune better this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "We had form 10 to 100 n_estimators\n",
    "and from 10 to 1584\n",
    "Best parameters: {'max_depth': 1584, 'n_estimators': 100}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000 1165 1359 1584]\n",
      "[  1000   4641  21544 100000]\n"
     ]
    }
   ],
   "source": [
    "# Parameters of the grid search\n",
    "n_estim_range = np.round(np.logspace(3,3.2,num=4,dtype=int))\n",
    "max_depth_range = np.round(np.logspace(3,5,num=4,dtype=int))\n",
    "print(n_estim_range)\n",
    "print(max_depth_range)\n",
    "n_estim_range = [200 500 900]\n",
    "max_depth_range = [300 500 700 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10 100]\n",
      "[  10   54  292 1584]\n"
     ]
    }
   ],
   "source": [
    "paramgrid = {'n_estimators': n_estim_range,'max_depth': max_depth_range}\n",
    "\n",
    "# Grid search on the number of estimators\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=rand_state), paramgrid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters:',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'pickles/'\n",
    "\n",
    "def save_results(var_name, file_name):\n",
    "    file_path = DATA_FOLDER + file_name +'.pickle'\n",
    "    my_file = Path(file_path)\n",
    "    my_dir = Path(DATA_FOLDER)\n",
    "    if not(my_dir.is_dir()):\n",
    "        os.makedirs(DATA_FOLDER)\n",
    "        save_results(var_name, file_name)\n",
    "    elif my_file.is_file():\n",
    "        print('WARNING! This filename already exusted so we wrote in \"overrided.pickle\" PLEASE MANUALLY CHANGE THE NAME'+\\\n",
    "             '\\n CHancge the file name with your name')\n",
    "        with open(DATA_FOLDER + 'overrided.pickle', 'wb') as file:\n",
    "            pickle.dump(var_name, file)\n",
    "    else:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(var_name, file)\n",
    "\n",
    "def read_pickle(file_name):\n",
    "    file_path = DATA_FOLDER + str(file_name) + '.pickle'\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        out = pickle.load(file)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This filename already exusted so we wrote in \"overrided.pickle\" PLEASE MANUALLY CHANGE THE NAME\n",
      " CHancge the file name with your name\n"
     ]
    }
   ],
   "source": [
    "save_results(grid_search, 'first_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 1584, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "lala = read_pickle('first_try')\n",
    "print('Best parameters:',lala.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.830769230769\n",
      "Validation score: 0.82599469496\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print('Train score:',grid_search.score(X_train, y_train))\n",
    "print('Test score:',grid_search.score(X_test, y_test))\n",
    "print('Validation score:',grid_search.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
